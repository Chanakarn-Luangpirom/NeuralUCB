{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "641fb798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "d8992854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedforwardNN(\n",
      "  (layer1): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (output_layer): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n",
      "Output shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size) #bias = False?\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.layer1(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define synthetic input data\n",
    "input_size = 20\n",
    "synthetic_input = torch.rand(input_size)\n",
    "\n",
    "# Instantiate the neural network\n",
    "hidden_size = 20\n",
    "output_size = 1\n",
    "model = FeedforwardNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Forward pass\n",
    "output = model(synthetic_input)\n",
    "\n",
    "# Print the architecture and output shape\n",
    "print(model)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "2c14d186",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "8eb179ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")          # a CUDA device object\n",
    "#     print('Using GPU:', torch.cuda.get_device_name(0))\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")           # a CPU device object\n",
    "#     print('CUDA is not available, using CPU.')\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "29f2b153",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: layer1.weight, Size: torch.Size([20, 20])\n",
      "Layer: output_layer.weight, Size: torch.Size([1, 20])\n",
      "Number of parameters: 420\n"
     ]
    }
   ],
   "source": [
    "p = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        x,y = param.size()\n",
    "        p += x*y\n",
    "        print(f'Layer: {name}, Size: {param.size()}')\n",
    "print('Number of parameters:',p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "7ecf0156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183.07807192206383"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulated Reward Function\n",
    "a = torch.rand(20)\n",
    "def reward_function(context):\n",
    "    return 10*(torch.dot(context,a)**2).item() + torch.randn(1).item() \n",
    "reward_function(torch.rand(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "f972c8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, predicted, target,flattened_initial_weights,flattened_current_weights):\n",
    "        loss = (0.5*torch.sum((predicted - target)**2))/target.shape[0]\n",
    "        m = 20\n",
    "        lamb = 1\n",
    "        norm = 0.5*m*lamb*(torch.norm(flattened_initial_weights - flattened_current_weights, p=2)**2)\n",
    "        return loss + norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "c1ac7cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(input_data,initial_weights,J = 20):\n",
    "    model_inside = FeedforwardNN(input_size, hidden_size, output_size)\n",
    "    model_inside.load_state_dict(initial_weights)\n",
    "\n",
    "    loss_function = CustomLoss()\n",
    "\n",
    "    optimizer = optim.SGD(model_inside.parameters(), lr=0.001) ## Fix to just GD on whole dataset and check if it's correct\n",
    "\n",
    "    size = len(input_data)\n",
    "    rewards = []\n",
    "    for s in range(size):\n",
    "        r = reward_function(input_data[s])\n",
    "        rewards.append(r)\n",
    "    rewards = torch.tensor(rewards).reshape(-1,1)\n",
    "    dataset = TensorDataset(input_data,rewards)\n",
    "\n",
    "    batch_size = 32\n",
    "    shuffle = True  # Set to True if you want to shuffle the data\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)    \n",
    "\n",
    "    flattened_initial_weights = torch.cat([p.flatten() for p in initial_weights.values()])\n",
    "    \n",
    "    for j in range(J):\n",
    "        for inputs,targets in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model_inside(inputs)\n",
    "            \n",
    "            current_weights = model_inside.state_dict()\n",
    "            flattened_current_weights = torch.cat([p.flatten() for p in current_weights.values()])\n",
    "            loss = loss_function(output, targets,flattened_initial_weights,flattened_current_weights)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "#         print('output',output)\n",
    "#         print('rewards',rewards)\n",
    "    print('--------------------------------------')\n",
    "\n",
    "    model_weights = model_inside.state_dict()\n",
    "    return model_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "25b92b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NeuralUCB(model, T, lamb = 0.2, v = 0.2, delta = 0.2, norm = 0.2, step_size = 0.2, num_iter = 10):\n",
    "    Z = torch.eye(p)\n",
    "    K = 2\n",
    "    dim = 20 # feature vector dimension (context)\n",
    "    m = 20 # Neural Network width\n",
    "    gamma = 0.2\n",
    "    best_contexts = []\n",
    "    for t in range(T):\n",
    "        synthetic_input = torch.rand((K,dim)) # 2 arms x 10 feature size\n",
    "        all_grad = []\n",
    "        ucbs = []\n",
    "        for k in range(K):\n",
    "            output = model(synthetic_input[k])\n",
    "            model.zero_grad()\n",
    "            output.backward(torch.ones_like(output),retain_graph = True)\n",
    "            g = []\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    tmp = param.grad.flatten()\n",
    "                    g.append(tmp)\n",
    "            g = torch.cat(g,dim = 0).reshape(-1,1)\n",
    "    \n",
    "    \n",
    "            Z_inv = torch.inverse(Z)\n",
    "            \n",
    "            \n",
    "            all_grad.append(g)\n",
    "            exploration_reward = gamma*torch.sqrt((g.T@Z_inv@g)/m) \n",
    "            ucb_a = output + exploration_reward\n",
    "            ucbs.append(ucb_a.item())\n",
    "            \n",
    "            \n",
    "        ucbs = torch.tensor(ucbs)\n",
    "        action = torch.argmax(ucbs).item()\n",
    "        best_context = synthetic_input[action]\n",
    "        best_g = all_grad[action]\n",
    "        Z = Z + (best_g@best_g.T/m)\n",
    "        \n",
    "        best_contexts.append(best_context)\n",
    "        \n",
    "#         if (t+1)%50 == 0:\n",
    "#             ### Train Neural Network\n",
    "#             new_weights = train_nn(torch.stack(best_contexts),initial_weights,J = t+1)\n",
    "#             model.load_state_dict(new_weights)\n",
    "#             print('abc')\n",
    "        \n",
    "        new_weights = train_nn(torch.stack(best_contexts),initial_weights,J = 20)\n",
    "        model.load_state_dict(new_weights)\n",
    "        # Update gamma: To be updated\n",
    "        print(t)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "144eb333",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "0\n",
      "--------------------------------------\n",
      "1\n",
      "--------------------------------------\n",
      "2\n",
      "--------------------------------------\n",
      "3\n",
      "--------------------------------------\n",
      "4\n",
      "--------------------------------------\n",
      "5\n",
      "--------------------------------------\n",
      "6\n",
      "--------------------------------------\n",
      "7\n",
      "--------------------------------------\n",
      "8\n",
      "--------------------------------------\n",
      "9\n",
      "--------------------------------------\n",
      "10\n",
      "--------------------------------------\n",
      "11\n",
      "--------------------------------------\n",
      "12\n",
      "--------------------------------------\n",
      "13\n",
      "--------------------------------------\n",
      "14\n",
      "--------------------------------------\n",
      "15\n",
      "--------------------------------------\n",
      "16\n",
      "--------------------------------------\n",
      "17\n",
      "--------------------------------------\n",
      "18\n",
      "--------------------------------------\n",
      "19\n",
      "--------------------------------------\n",
      "20\n",
      "--------------------------------------\n",
      "21\n",
      "--------------------------------------\n",
      "22\n",
      "--------------------------------------\n",
      "23\n",
      "--------------------------------------\n",
      "24\n",
      "--------------------------------------\n",
      "25\n",
      "--------------------------------------\n",
      "26\n",
      "--------------------------------------\n",
      "27\n",
      "--------------------------------------\n",
      "28\n",
      "--------------------------------------\n",
      "29\n",
      "--------------------------------------\n",
      "30\n",
      "--------------------------------------\n",
      "31\n",
      "--------------------------------------\n",
      "32\n",
      "--------------------------------------\n",
      "33\n",
      "--------------------------------------\n",
      "34\n",
      "--------------------------------------\n",
      "35\n",
      "--------------------------------------\n",
      "36\n",
      "--------------------------------------\n",
      "37\n",
      "--------------------------------------\n",
      "38\n",
      "--------------------------------------\n",
      "39\n",
      "--------------------------------------\n",
      "40\n",
      "--------------------------------------\n",
      "41\n",
      "--------------------------------------\n",
      "42\n",
      "--------------------------------------\n",
      "43\n",
      "--------------------------------------\n",
      "44\n",
      "--------------------------------------\n",
      "45\n",
      "--------------------------------------\n",
      "46\n",
      "--------------------------------------\n",
      "47\n",
      "--------------------------------------\n",
      "48\n",
      "--------------------------------------\n",
      "49\n",
      "--------------------------------------\n",
      "50\n",
      "--------------------------------------\n",
      "51\n",
      "--------------------------------------\n",
      "52\n",
      "--------------------------------------\n",
      "53\n",
      "--------------------------------------\n",
      "54\n",
      "--------------------------------------\n",
      "55\n",
      "--------------------------------------\n",
      "56\n",
      "--------------------------------------\n",
      "57\n",
      "--------------------------------------\n",
      "58\n",
      "--------------------------------------\n",
      "59\n",
      "--------------------------------------\n",
      "60\n",
      "--------------------------------------\n",
      "61\n",
      "--------------------------------------\n",
      "62\n",
      "--------------------------------------\n",
      "63\n",
      "--------------------------------------\n",
      "64\n",
      "--------------------------------------\n",
      "65\n",
      "--------------------------------------\n",
      "66\n",
      "--------------------------------------\n",
      "67\n",
      "--------------------------------------\n",
      "68\n",
      "--------------------------------------\n",
      "69\n",
      "--------------------------------------\n",
      "70\n",
      "--------------------------------------\n",
      "71\n",
      "--------------------------------------\n",
      "72\n",
      "--------------------------------------\n",
      "73\n",
      "--------------------------------------\n",
      "74\n",
      "--------------------------------------\n",
      "75\n",
      "--------------------------------------\n",
      "76\n",
      "--------------------------------------\n",
      "77\n",
      "--------------------------------------\n",
      "78\n",
      "--------------------------------------\n",
      "79\n",
      "--------------------------------------\n",
      "80\n",
      "--------------------------------------\n",
      "81\n",
      "--------------------------------------\n",
      "82\n",
      "--------------------------------------\n",
      "83\n",
      "--------------------------------------\n",
      "84\n",
      "--------------------------------------\n",
      "85\n",
      "--------------------------------------\n",
      "86\n",
      "--------------------------------------\n",
      "87\n",
      "--------------------------------------\n",
      "88\n",
      "--------------------------------------\n",
      "89\n",
      "--------------------------------------\n",
      "90\n",
      "--------------------------------------\n",
      "91\n",
      "--------------------------------------\n",
      "92\n",
      "--------------------------------------\n",
      "93\n",
      "--------------------------------------\n",
      "94\n",
      "--------------------------------------\n",
      "95\n",
      "--------------------------------------\n",
      "96\n",
      "--------------------------------------\n",
      "97\n",
      "--------------------------------------\n",
      "98\n",
      "--------------------------------------\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "model = NeuralUCB(model,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708a694e",
   "metadata": {},
   "source": [
    "### Result Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "890a2951",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.rand((2,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "a24b7c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3688)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "1e6212c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.7209)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "fae6dff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx_0 reward:  184.6775421500206\n",
      "idx_1 reward:  239.36793792247772\n"
     ]
    }
   ],
   "source": [
    "best_idx = 0\n",
    "best_reward = 0\n",
    "for idx,t in enumerate(test):\n",
    "    reward = reward_function(t)\n",
    "    if reward>best_reward:\n",
    "        best_reward = reward\n",
    "        best_idx = idx\n",
    "    print(f'idx_{idx} reward: ',reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "d3b4cdfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[178.0201],\n",
       "        [239.3335]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
